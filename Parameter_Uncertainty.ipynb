{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parameter Uncertainty",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1Mc6Np1UKMlW09b0vDpBJBRA8miAv3Lmw",
      "authorship_tag": "ABX9TyO/9nqWvTPxPvZ2lv8o/yqV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mku813/CAU_paper/blob/main/Parameter_Uncertainty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADVmBzeLlI4_"
      },
      "source": [
        "https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whvwPVyl1bdT"
      },
      "source": [
        "## 3.Dropout Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1wSZaKh0UYo"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import h5py\n",
        "from scipy.ndimage.interpolation import rotate\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import pymc3 as pm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFFEVtXn1oYX"
      },
      "source": [
        "## 2.Dropout Implementation\n",
        "\n",
        "*   drop된 뉴런에 대해 보상하기 위해서 multiplier를 곱해준다.\n",
        "*   첫 번째 example에서만 `MyDropout`을 사용할 것이고, 그 이후부터는 `nn.Dropout`을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lV6kXz01h8b"
      },
      "source": [
        "class MyDropout(nn.Module):\n",
        "  def __init__(self, p=0.5):\n",
        "    super(MyDropout, self).__init__()\n",
        "    self.p = p\n",
        "\n",
        "    # multiplier is 1/(1-p)\n",
        "    # Set multiplier to 0 when p = 1 to avoid error\n",
        "    if self.p < 1:\n",
        "      self.multiplier_ = 1.0 / (1.0 - p)\n",
        "    else:\n",
        "      self.multiplier_ = 0.0\n",
        "    \n",
        "  def forward(self, input):\n",
        "    # if model.eval(), don't apply dropout\n",
        "    if not self.training:\n",
        "      return input\n",
        "    \n",
        "    # So that we have `input.shape` numbers of Bernoulli(1-p) samples\n",
        "    # --> input의 데이터 사이즈를 고려하여 베르누이(1-p)의 샘플을 만들기 위한 과정\n",
        "    selected_ = torch.Tensor(input.shape).uniform_(0, 1) > self.p\n",
        "    \n",
        "    # To suppert both CPU and GPU\n",
        "    if input.is_cuda:\n",
        "      selected_ = Variable(selected_.type(torch.cuda.FloatTensor), requires_grad = False)\n",
        "    else:\n",
        "      selected_ = Variable(selected_.type(torch.FloatTensor), requires_grad = False)\n",
        "\n",
        "    # Multiply output by multiplier as described in the paper [1]\n",
        "    return torch.mul(selected_, input)*self.multiplier_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgkHKeY86B9O"
      },
      "source": [
        "## 3.Dropout as Regularization\n",
        "\n",
        "* 딥뉴럴넷에서 regularization으로 작동하는 dropout을 보려고 한다.(L1, L2와 같은)\n",
        "* Dropout이 작동하는 것을 확인하기 위해 fully connected network인 multilayer perceptron을 만들고, 그 후에 dropout이 다른 네트워크 아키텍쳐에서도 잘 작동하는지 확인하기 위해 LeNet(CNN 계열)을 만들 것이다.\n",
        "* MNIST를 사용할 것이다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXpZ_gpLcwJa"
      },
      "source": [
        "# Normalize data with mean=(0,0,0), std=(1,1,1)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    transforms.Normalize((0,), (1,))\n",
        "])\n",
        "\n",
        "path = './drive/MyDrive/paper/data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJHoVP155-2E"
      },
      "source": [
        "trainset = datasets.MNIST(root=path,   # root directory of dataset where `MNIST/processed/[training.pt | test.pt]`\n",
        "                          train=True,     # if True, creates dataset from training.pt, otherwise from test.pt\n",
        "                          download=True,  # if True, downloads from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.\n",
        "                          transform=transform   # A function/transform that takes in an PIL image and returns a tranformed version. E.g, `transforms.RandomCrop`\n",
        "                          # target_tranform=    # A function/transform that takes in the target and transforms it.\n",
        "                          )\n",
        "testset = datasets.MNIST(root=path, train=False, download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3YfvCPOeTQD"
      },
      "source": [
        "print(trainset)\n",
        "print(testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMNCJJYPHT_G"
      },
      "source": [
        "# Visualize 10 images samples in MNIST dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "data_iter = iter(trainloader)\n",
        "images, labels = data_iter.next()\n",
        "\n",
        "# plot 10 sample images\n",
        "_, ax = plt.subplots(1, 10)\n",
        "ax = ax.flatten()\n",
        "iml = images[0].numpy().shape[1]\n",
        "[ax[i].imshow(np.transpose(images[i].numpy(), (1,2,0)).reshape(iml, -1), cmap='Greys') for i in range(10)]\n",
        "[ax[i].set_axis_off() for i in range(10)]\n",
        "plt.show()\n",
        "\n",
        "print('lable:', labels[:10].numpy())\n",
        "print('image data shape:', images[0].numpy().shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8x1M4hWkIxi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP0NzyNsDTBp"
      },
      "source": [
        "### 4.1 Multilayer Perceptron\n",
        "\n",
        "* 기본적인 MLP(MultiLayer Perceptron)을 만들고자 한다.\n",
        "  * hidden layer: 2개\n",
        "  * hidden unit: 800개\n",
        "\n",
        "<br>\n",
        "\n",
        "* sklearn과 비슷한 classifier를 만들어보고자 한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "* 논문에 보면 3가지 실험이 있다.\n",
        "  * no dropout\n",
        "  * dropout(0.5)\n",
        "  * dropout(0.5), input(0.2)\n",
        "* 시간이 오래 걸리므로 GPU로 하는 것을 권장함.\n",
        "\n",
        "<br>\n",
        "\n",
        "* plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-03AdS7-vAU"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, hidden_layers=[800, 800], dropRates=[0,0]):\n",
        "    super(MLP, self).__init__()\n",
        "    self.model = nn.Sequential()\n",
        "    self.model.add_module(\"dropout0\", MyDropout(p=dropRates[0]))\n",
        "    self.model.add_module(\"input\", nn.Linear(28*28, hidden_layers[0]))\n",
        "    self.model.add_module(\"tanh\", nn.Tanh())\n",
        "\n",
        "    # Add hidden layer\n",
        "    for i, d in enumerate(hidden_layers[:-1]):\n",
        "      self.model.add_module(\"dropout_hidden\"+str(i+1), MyDropout(p=dropRates[1]))\n",
        "      self.model.add_module(\"hidden\"+str(i+1), nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n",
        "      self.model.add_module(\"tanh_hidden\"+str(i+1), nn.Tanh())\n",
        "    self.model.add_module(\"final\", nn.Linear(hidden_layers[-1], 10))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Trun to 1-Dimension\n",
        "    x = x.view(x.shape[0], 28*28)\n",
        "    x = self.model(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljLXC9QZFS0n"
      },
      "source": [
        "class MLPClassifier:\n",
        "  def __init__(self, hidden_layer=[800, 800], dropRates=[0,0], batch_size=128, max_epoch=10, lr=0.1, momentum=0):\n",
        "    # Wrap MLP model\n",
        "    self.hidden_layer = hidden_layer\n",
        "    self.dropRates = dropRates\n",
        "    self.batch_size = batch_size\n",
        "    self.max_epoch = max_epoch\n",
        "\n",
        "    self.model = MLP(hidden_layers=hidden_layers, dropRates=dropRates)\n",
        "    self.model.cuda()\n",
        "    self.criterion = nn.CrossEntropyLoss().cuda()\n",
        "    self.optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "    self.loss_ = []\n",
        "    self.test_acc = []\n",
        "    self.test_err = []\n",
        "  \n",
        "  def fit(self, trainset, testset, verbose=True):\n",
        "    # GPU 확인하고 진행하길,, 안그러면 매우 느릴수 있음\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=self.batch_size, shuffle=True)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
        "    X_test, y_test = iter(testloader).next()\n",
        "    X_test = X_test.cuda()\n",
        "\n",
        "    for epoch in range(self.max_epoch):\n",
        "      running_loss = 0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        running_loss += loss.data.cpu().numpy()\n",
        "      self.loss_.append(running_loss / len(trainloader))\n",
        "\n",
        "      if verbose:\n",
        "        print('Epoch {} loss: {}'.format(epoch+1, self.loss_[-1]))\n",
        "      \n",
        "      y_test_pred = self.predict(X_test).cpu()\n",
        "      tmp_test_acc = y_test == y_test_pred\n",
        "      self.test_acc.append(np.mean(tmp_test_acc.data.cpu().numpy()))\n",
        "      # self.test_acc.append(np.mean(y_test == y_test_pred))\n",
        "      self.test_err.append(int(len(testset)*(1-self.test_acc[-1])))\n",
        "\n",
        "      if verbose:\n",
        "        print('Test error: {}; test acc: {}'.format(self.test_err[-1], self.test_acc[-1]))\n",
        "    \n",
        "    return self\n",
        "\n",
        "  def predict(self, x):\n",
        "    # Used to keep all test errors after each epoch\n",
        "    model = self.model.eval()\n",
        "    outputs = model(Variable(x))\n",
        "    _, pred = torch.max(outputs.data, 1)\n",
        "    model = self.model.train()\n",
        "    return pred\n",
        "  \n",
        "  def __str__(self):\n",
        "    return 'Hidden layers: {}; dropout rate: {}'.format(self.hidden_layers, self.dropRates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhUtBGvnUP6V"
      },
      "source": [
        "hidden_layers = [800, 800]\n",
        "\n",
        "### Below is training code, uncomment to train your own model... ###\n",
        "### Note: You need GPU to run this section ###\n",
        "\n",
        "# Define networks\n",
        "mlp1 = [MLPClassifier(hidden_layers, dropRates=[0, 0], max_epoch=300), \n",
        "        MLPClassifier(hidden_layers, dropRates=[0, 0.5], max_epoch=300),\n",
        "        MLPClassifier(hidden_layers, dropRates=[0.2, 0.5], max_epoch=300)]\n",
        "        \n",
        "# Training, set verbose=True to see loss after each epoch.\n",
        "[mlp.fit(trainset, testset,verbose=True) for mlp in mlp1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRN-fTzvJDos"
      },
      "source": [
        "# Save torch models\n",
        "for ind, mlp in enumerate(mlp1):\n",
        "    torch.save(mlp.model, './drive/MyDrive/paper/mnist_mlp1_'+str(ind)+'.pth')\n",
        "    # Prepare to save errors\n",
        "    mlp.test_error = list(map(str, mlp.test_err))\n",
        "\n",
        "# Save test errors to plot figures\n",
        "open(\"./drive/MyDrive/paper/mlp1_test_errors.txt\",\"w\").write('\\n'.join([','.join(mlp.test_error) for mlp in mlp1])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmZcExIso_Cv"
      },
      "source": [
        "# Load saved models to CPU\n",
        "mlp1_models = [torch.load('./drive/MyDrive/paper/mnist_mlp1_'+str(ind)+'.pth',map_location={'cuda:0': 'cpu'}) for ind in [0,1,2]]\n",
        "\n",
        "# Load saved test errors to plot figures.\n",
        "mlp1_test_errors = [error_array.split(',') for error_array in open(\"./drive/MyDrive/paper/mlp1_test_errors.txt\",\"r\").read().split('\\n')]\n",
        "mlp1_test_errors = np.array(mlp1_test_errors,dtype='f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL0OdXbSMDpo"
      },
      "source": [
        "label = ['MLP no dropout',\n",
        "         'MLP 50% dropout in hidden layers',\n",
        "         'MLP 50% dropout in hidden layers + 20% in input layer']\n",
        "\n",
        "plt.figure(figsize=(8,7))\n",
        "for i, r in enumerate(mlp1_test_errors):\n",
        "  plt.plot(range(1, len(r)+1), r, '.-', label=labels[i], alpha=0.6)\n",
        "# plt.ylim([50,250])\n",
        "plt.legend(loc=1)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Number of errors in test set')\n",
        "plt.title('Test Error on MNIST dataset for MLP')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAMB2h2sM3Z6"
      },
      "source": [
        "### 4.2 CNN with LeNet\n",
        "\n",
        "* LeNet에 대한 간략한 소개 후 Dropout이 test 성능에 더 좋은 결과를 가져온다는 것을 보여줄것이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dqH1kUTvD9A"
      },
      "source": [
        "- 일단은 conv layer와 pooling layer를 통과한 후 이미지의 dimension을 계산해야한다.\n",
        "- pytorch의 linear layer 때문에 해야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1f3KHirM8w9"
      },
      "source": [
        "def caloutdim(hin, kernel_size, stride=1, padding=0, dilation=1):\n",
        "    return int(np.floor((hin+2*padding-dilation*(kernel_size-1)-1)/stride+1))\n",
        "\n",
        "d = [28]\n",
        "d.append(caloutdim(d[-1], 5, padding=2))\n",
        "d.append(caloutdim(d[-1], 2, 2))\n",
        "d.append(caloutdim(d[-1], 5, padding=2))\n",
        "d.append(caloutdim(d[-1], 2, 2))\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcY9UvKTvcKQ"
      },
      "source": [
        "- 아래의 코드는 LeNet의 코드이다.\n",
        "- 여기에 `nn.Dropout2d`를 사용했다.\n",
        "- 이것은 `MyDropout`과 같은 것이다.\n",
        "  - 2차원인 것 빼고\n",
        "  - 효율은 더 좋다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-mZlF9FpWVl"
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, droprate=0.5):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.model = nn.Sequential()\n",
        "        self.model.add_module('conv1', nn.Conv2d(1, 20, kernel_size=5, padding=2))\n",
        "        self.model.add_module('dropout1', nn.Dropout2d(p=droprate))\n",
        "        self.model.add_module('maxpool1', nn.MaxPool2d(2, stride=2))\n",
        "        self.model.add_module('conv2', nn.Conv2d(20, 50, kernel_size=5, padding=2))\n",
        "        self.model.add_module('dropout2', nn.Dropout2d(p=droprate))\n",
        "        self.model.add_module('maxpool2', nn.MaxPool2d(2, stride=2))\n",
        "        self.model.add_module('flatten', Flatten())\n",
        "        self.model.add_module('dense3', nn.Linear(50*7*7, 500))\n",
        "        self.model.add_module('relu3', nn.ReLU())\n",
        "        self.model.add_module('dropout3', nn.Dropout(p=droprate))\n",
        "        self.model.add_module('final', nn.Linear(500, 10))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVGu5SR5v8ho"
      },
      "source": [
        "- 위의 내용과 비슷하지만 sklearn을 만든 LeNet Classifier다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq7R3R4kpYmt"
      },
      "source": [
        "class LeNetClassifier:\n",
        "    def __init__(self, droprate=0.5, batch_size=128, max_epoch=300, lr=0.01):\n",
        "        self.batch_size = batch_size\n",
        "        self.max_epoch = max_epoch\n",
        "        self.lr = lr\n",
        "        self.model = LeNet(droprate)\n",
        "        self.model.cuda()\n",
        "        self.criterion = nn.CrossEntropyLoss().cuda()\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
        "        self.loss_ = []\n",
        "        self.test_error = []\n",
        "        self.test_accuracy = []\n",
        "        \n",
        "    def fit(self, trainset, testset, verbose=True):\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=self.batch_size, shuffle=True)\n",
        "        testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
        "        X_test, y_test = iter(testloader).next()\n",
        "        X_test = X_test.cuda()\n",
        "        print(self.model)\n",
        "        for epoch in range(self.max_epoch):\n",
        "            running_loss = 0\n",
        "            for i, data in enumerate(trainloader, 0):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                running_loss += loss.data.cpu().numpy()\n",
        "            self.loss_.append(running_loss / len(trainloader))\n",
        "            if verbose:\n",
        "                print('Epoch {} loss: {}'.format(epoch+1, self.loss_[-1]))\n",
        "            y_test_pred = self.predict(X_test).cpu()\n",
        "\n",
        "            tmp_test_accuracy = y_test == y_test_pred\n",
        "            self.test_accuracy.append(np.mean(tmp_test_accuracy.data.cpu().numpy()))\n",
        "            # self.test_accuracy.append(np.mean(y_test == y_test_pred))\n",
        "            self.test_error.append(int(len(testset)*(1-self.test_accuracy[-1])))\n",
        "            if verbose:\n",
        "                print('Test error: {}; test accuracy: {}'.format(self.test_error[-1], self.test_accuracy[-1]))\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x):\n",
        "        model = self.model.eval()\n",
        "        outputs = model(Variable(x))\n",
        "        _, pred = torch.max(outputs.data, 1)\n",
        "        model = self.model.train()\n",
        "        return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrYxPzEew4kZ"
      },
      "source": [
        "- 아래 코드는 training code이고 학습시킨 모델을 로딩할 것이다.\n",
        "- training 시간이 오래걸린다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2LP01USSDhu"
      },
      "source": [
        "### Below is training code, uncomment to train your own model... ###\n",
        "### Note: You need GPU and CUDA to run this section ###\n",
        "\n",
        "# Define networks\n",
        "lenet1 = [LeNetClassifier(droprate=0, max_epoch=300),\n",
        "          LeNetClassifier(droprate=0.5, max_epoch=300)]\n",
        "        \n",
        "# Training, set verbose=True to see loss after each epoch.\n",
        "[lenet.fit(trainset, testset,verbose=True) for lenet in lenet1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H5RK411SGJv"
      },
      "source": [
        "# Save torch models\n",
        "for ind, lenet in enumerate(lenet1):\n",
        "    torch.save(lenet.model, './drive/MyDrive/paper/mnist_lenet1_'+str(ind)+'.pth')\n",
        "    # Prepare to save errors\n",
        "    lenet.test_error = list(map(str, lenet.test_error))\n",
        "\n",
        "# Save test errors to plot figures\n",
        "open(\"./drive/MyDrive/paper/lenet1_test_errors.txt\",\"w\").write('\\n'.join([','.join(lenet.test_error) for lenet in lenet1])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkVpOekSpbeX"
      },
      "source": [
        "# Load saved models to CPU\n",
        "lenet1_models = [torch.load('./drive/MyDrive/paper/mnist_lenet1_'+str(ind)+'.pth', map_location={'cuda:0': 'cpu'}) for ind in [0,1]]\n",
        "\n",
        "# Load saved test errors to plot figures.\n",
        "lenet1_test_errors = [error_array.split(',') for error_array in \n",
        "                      open(\"./drive/MyDrive/paper/lenet1_test_errors.txt\",\"r\").read().split('\\n')]\n",
        "lenet1_test_errors = np.array(lenet1_test_errors,dtype='f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g24uJ3W3xjNB"
      },
      "source": [
        "- 학습 후 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9mLQyXwpdKm"
      },
      "source": [
        "labels = ['MLP no dropout', \n",
        "          'MLP 50% dropout in hidden layers', \n",
        "          'MLP 50% dropout in hidden layers + 20% in input layer',\n",
        "          'LeNet no dropout',\n",
        "          'LeNet 50% dropout']\n",
        "\n",
        "plt.figure(figsize=(8, 7))\n",
        "for i, r in enumerate(mlp1_test_errors.tolist() + lenet1_test_errors.tolist()):\n",
        "    plt.plot(range(1, len(r)+1), r, '.-', label=labels[i], alpha=0.6);\n",
        "plt.ylim([50, 250]);\n",
        "plt.legend(loc=1);\n",
        "plt.xlabel('Epochs');\n",
        "plt.ylabel('Number of errors in test set');\n",
        "plt.title('Test Error on MNIST Dataset for All Networks')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgE-agyTHeUZ"
      },
      "source": [
        "#### get parameter\n",
        "\n",
        "모델의 각 parameter들을 구할 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd3EfxJzHqA5"
      },
      "source": [
        "for name, param in lenet1_models[0].named_parameters():\n",
        "  if param.requires_grad:\n",
        "    print(name, param.data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1m-kJ_REqBQ"
      },
      "source": [
        "for name, param in lenet1_models[1].named_parameters():\n",
        "  if param.requires_grad:\n",
        "    print(name, param.data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnrn652gAOS"
      },
      "source": [
        "## 5.Dropout as Bayesian Approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz1cUgj_gOfo"
      },
      "source": [
        "### 5.1 Dropout as Bayesian Approximation in Classification Task\n",
        "\n",
        "- 이제 두 모델(MLP와 LeNet)에서 dropout을 사용하여 모델 불확실성을 구하는지 진행해 볼 것이다.\n",
        "- 일단 숫자 1을 조금씩 돌려가며 12개의 이미지를 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYN5iuQyFPHV"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
        "X_test, y_test = iter(testloader).next()\n",
        "X_test = X_test.numpy()\n",
        "X1 = np.array([rotate(X_test[9978].squeeze(), i, reshape=False) for i in range(50, 130, 7)])\n",
        "X1 = X1.reshape(X1.shape[0], 1, X1.shape[1], X1.shape[2])\n",
        "\n",
        "plt.figure(figsize=(8, 1))\n",
        "\n",
        "gs = gridspec.GridSpec(1, 12)\n",
        "gs.update(wspace=0, hspace=0)\n",
        "\n",
        "for i in range(len(X1)):\n",
        "    plt.subplot(gs[i])\n",
        "    plt.imshow(X1.squeeze()[i], cmap='gray');\n",
        "    plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv4AtP-ahqWS"
      },
      "source": [
        "- 각각의 모델들에 대해 1000번의 시뮬레이션을 진행했고, softmax의 input과 output의 분포를 가장 정답이라고 생각하는 클래스에 대해 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FY2jA44x2q2"
      },
      "source": [
        "def predict_class(model, X):\n",
        "    model = model.eval()\n",
        "    outputs = model(Variable(X))\n",
        "    print('outputs.data: \\n{}'.format(outputs.data))\n",
        "    _, pred = torch.max(outputs.data, 1)\n",
        "    print('_, pred: \\n{}, {}'.format(_, pred))\n",
        "    model = model.train()\n",
        "    return pred.numpy()\n",
        "\n",
        "def predict(model, X, T=1000):\n",
        "    standard_pred = predict_class(model, X)\n",
        "    y1 = []\n",
        "    y2 = []\n",
        "    for _ in range(T):\n",
        "        _y1 = model(Variable(X))\n",
        "        _y2 = F.softmax(_y1, dim=1)\n",
        "        y1.append(_y1.data.numpy())\n",
        "        y2.append(_y2.data.numpy())\n",
        "    return standard_pred, np.array(y1), np.array(y2)    # return pred, softmax input, softmax output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHCW0XvYk_bH"
      },
      "source": [
        "#### 5.1.1 MLP 50% dropout in hidden layers + 20% in input layer\n",
        "\n",
        "- dropout을 제목과 같이 적용하였다\n",
        "  - 일단은 숫자 `1`만 보도록 하자(12개의 rotate된 숫자 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M7TI2gvx439"
      },
      "source": [
        "model = mlp1_models[2]\n",
        "\n",
        "# Need to flatten X1 before feeding into MLP\n",
        "y1_pred, y1_si, y1_so = predict(model, torch.from_numpy(X1.reshape(-1,784)))    # si: softmax input, so:softmax output\n",
        "print('Predictions: {}'.format(y1_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7T1k590nZS4"
      },
      "source": [
        "- 위에서 나온 class의 분포를 softmax input 값과 output값들에 대하여 그려놓은 것이다.\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3uJtWEyx6od"
      },
      "source": [
        "plt.figure(figsize=(10, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_si.shape[0]), y1_si[:, :, 1].flatten(), \\\n",
        "            color='g', marker='_', linewidth=None, alpha=0.2, label='1');\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_si.shape[0]), y1_si[:, :, 7].flatten(), \\\n",
        "            color='r', marker='_', linewidth=None, alpha=0.2, label='7');\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_si.shape[0]), y1_si[:, :, 3].flatten(), \\\n",
        "            color='b', marker='_', linewidth=None, alpha=0.2, label='3');\n",
        "plt.title('Softmax input scatter');\n",
        "plt.legend(framealpha=0.7);\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_so.shape[0]), y1_so[:, :, 1].flatten(), \\\n",
        "            color='g', marker='_', linewidth=None, alpha=0.2, label='1');\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_so.shape[0]), y1_so[:, :, 7].flatten(), \\\n",
        "            color='r', marker='_', linewidth=None, alpha=0.2, label='7');\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_so.shape[0]), y1_so[:, :, 3].flatten(), \\\n",
        "            color='b', marker='_', linewidth=None, alpha=0.2, label='3');\n",
        "plt.title('Softmax output scatter');\n",
        "plt.legend(framealpha=0.7);\n",
        "\n",
        "plt.tight_layout();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLspgXSZx8QF"
      },
      "source": [
        "model = lenet1_models[1]\n",
        "y1_pred, y1_si, y1_so = predict(model, torch.from_numpy(X1))\n",
        "print('Predictions: {}'.format(y1_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EcGw4MQx93V"
      },
      "source": [
        "model = lenet1_models[1]\n",
        "y1_pred, y1_si, y1_so = predict(model, torch.from_numpy(X1))\n",
        "print('Predictions: {}'.format(y1_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL5lceQ0x_Ld"
      },
      "source": [
        "plt.figure(figsize=(10, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_si.shape[0]), y1_si[:, :, 1].flatten(), \\\n",
        "            color='g', marker='_', linewidth=None, alpha=0.2, label='1');\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_si.shape[0]), y1_si[:, :, 7].flatten(), \\\n",
        "            color='r', marker='_', linewidth=None, alpha=0.2, label='7');\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_si.shape[0]), y1_si[:, :, 3].flatten(), \\\n",
        "            color='b', marker='_', linewidth=None, alpha=0.2, label='3');\n",
        "plt.title('Softmax input scatter');\n",
        "plt.legend(framealpha=0.7);\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_so.shape[0]), y1_so[:, :, 1].flatten(), \\\n",
        "            color='g', marker='_', linewidth=None, alpha=0.2, label='1');\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_so.shape[0]), y1_so[:, :, 7].flatten(), \\\n",
        "            color='r', marker='_', linewidth=None, alpha=0.2, label='7');\n",
        "plt.scatter(np.tile(np.arange(1, 13), y1_so.shape[0]), y1_so[:, :, 3].flatten(), \\\n",
        "            color='b', marker='_', linewidth=None, alpha=0.2, label='3');\n",
        "plt.title('Softmax output scatter');\n",
        "plt.legend(framealpha=0.7);\n",
        "\n",
        "plt.tight_layout();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNM3Oxc-yAlm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}